{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from flaml import AutoML\n",
    "from flaml.ml import sklearn_metric_loss_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration (ms)</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>spec_rate</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>195000.0</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-8.815</td>\n",
       "      <td>0.0672</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.7530</td>\n",
       "      <td>0.520</td>\n",
       "      <td>128.050</td>\n",
       "      <td>3.446154e-07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194641.0</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.781</td>\n",
       "      <td>-6.848</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.009530</td>\n",
       "      <td>0.3490</td>\n",
       "      <td>0.250</td>\n",
       "      <td>122.985</td>\n",
       "      <td>1.464234e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>217573.0</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-8.029</td>\n",
       "      <td>0.0872</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.247</td>\n",
       "      <td>170.044</td>\n",
       "      <td>4.007850e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>443478.0</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.699</td>\n",
       "      <td>-4.571</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.0888</td>\n",
       "      <td>0.199</td>\n",
       "      <td>92.011</td>\n",
       "      <td>7.959809e-08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>225862.0</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.771</td>\n",
       "      <td>-5.863</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.3650</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0965</td>\n",
       "      <td>0.163</td>\n",
       "      <td>115.917</td>\n",
       "      <td>4.693131e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration (ms)  danceability  energy  loudness  speechiness  acousticness  \\\n",
       "0       195000.0         0.611   0.614    -8.815       0.0672        0.0169   \n",
       "1       194641.0         0.638   0.781    -6.848       0.0285        0.0118   \n",
       "2       217573.0         0.560   0.810    -8.029       0.0872        0.0071   \n",
       "3       443478.0         0.525   0.699    -4.571       0.0353        0.0178   \n",
       "4       225862.0         0.367   0.771    -5.863       0.1060        0.3650   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo     spec_rate  labels  \n",
       "0          0.000794    0.7530    0.520  128.050  3.446154e-07       2  \n",
       "1          0.009530    0.3490    0.250  122.985  1.464234e-07       1  \n",
       "2          0.000008    0.2410    0.247  170.044  4.007850e-07       1  \n",
       "3          0.000088    0.0888    0.199   92.011  7.959809e-08       0  \n",
       "4          0.000001    0.0965    0.163  115.917  4.693131e-07       1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"278k_song_labelled.csv\")\n",
    "df = df.loc[:, ~df.columns.str.contains(\"unnamed\", case=False)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration (ms)</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>spec_rate</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.779380e+05</td>\n",
       "      <td>277938.000000</td>\n",
       "      <td>277938.000000</td>\n",
       "      <td>277938.000000</td>\n",
       "      <td>277938.000000</td>\n",
       "      <td>277938.000000</td>\n",
       "      <td>277938.000000</td>\n",
       "      <td>277938.000000</td>\n",
       "      <td>277938.000000</td>\n",
       "      <td>277938.000000</td>\n",
       "      <td>2.779380e+05</td>\n",
       "      <td>277938.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.324961e+05</td>\n",
       "      <td>0.552583</td>\n",
       "      <td>0.556866</td>\n",
       "      <td>-10.363654</td>\n",
       "      <td>0.087913</td>\n",
       "      <td>0.386583</td>\n",
       "      <td>0.255044</td>\n",
       "      <td>0.189217</td>\n",
       "      <td>0.449602</td>\n",
       "      <td>119.196002</td>\n",
       "      <td>4.754654e-07</td>\n",
       "      <td>1.179101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.171830e+05</td>\n",
       "      <td>0.188905</td>\n",
       "      <td>0.279681</td>\n",
       "      <td>6.672049</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.364504</td>\n",
       "      <td>0.373745</td>\n",
       "      <td>0.163596</td>\n",
       "      <td>0.267471</td>\n",
       "      <td>30.462256</td>\n",
       "      <td>9.190229e-07</td>\n",
       "      <td>1.021033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.706000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.720130e+05</td>\n",
       "      <td>0.431000</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>-12.747000</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096200</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>95.072250</td>\n",
       "      <td>1.531461e-07</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.131055e+05</td>\n",
       "      <td>0.571000</td>\n",
       "      <td>0.591000</td>\n",
       "      <td>-8.397000</td>\n",
       "      <td>0.047100</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>0.434000</td>\n",
       "      <td>119.940000</td>\n",
       "      <td>2.345459e-07</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.648660e+05</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>-5.842000</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.754000</td>\n",
       "      <td>0.645000</td>\n",
       "      <td>0.227000</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>138.869750</td>\n",
       "      <td>4.449937e-07</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.919895e+06</td>\n",
       "      <td>0.989000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.882000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>244.947000</td>\n",
       "      <td>5.971860e-05</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       duration (ms)   danceability         energy       loudness  \\\n",
       "count   2.779380e+05  277938.000000  277938.000000  277938.000000   \n",
       "mean    2.324961e+05       0.552583       0.556866     -10.363654   \n",
       "std     1.171830e+05       0.188905       0.279681       6.672049   \n",
       "min     6.706000e+03       0.000000       0.000000     -60.000000   \n",
       "25%     1.720130e+05       0.431000       0.342000     -12.747000   \n",
       "50%     2.131055e+05       0.571000       0.591000      -8.397000   \n",
       "75%     2.648660e+05       0.693000       0.792000      -5.842000   \n",
       "max     3.919895e+06       0.989000       1.000000       4.882000   \n",
       "\n",
       "         speechiness   acousticness  instrumentalness       liveness  \\\n",
       "count  277938.000000  277938.000000     277938.000000  277938.000000   \n",
       "mean        0.087913       0.386583          0.255044       0.189217   \n",
       "std         0.112500       0.364504          0.373745       0.163596   \n",
       "min         0.000000       0.000000          0.000000       0.000000   \n",
       "25%         0.035900       0.033800          0.000000       0.096200   \n",
       "50%         0.047100       0.262000          0.001090       0.121000   \n",
       "75%         0.082200       0.754000          0.645000       0.227000   \n",
       "max         0.965000       0.996000          1.000000       1.000000   \n",
       "\n",
       "             valence          tempo     spec_rate         labels  \n",
       "count  277938.000000  277938.000000  2.779380e+05  277938.000000  \n",
       "mean        0.449602     119.196002  4.754654e-07       1.179101  \n",
       "std         0.267471      30.462256  9.190229e-07       1.021033  \n",
       "min         0.000000       0.000000  0.000000e+00       0.000000  \n",
       "25%         0.220000      95.072250  1.531461e-07       0.000000  \n",
       "50%         0.434000     119.940000  2.345459e-07       1.000000  \n",
       "75%         0.665000     138.869750  4.449937e-07       2.000000  \n",
       "max         1.000000     244.947000  5.971860e-05       3.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "1    106429\n",
       "0     82058\n",
       "2     47065\n",
       "3     42386\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cnt = df.shape[0]\n",
    "train_eval_split = int(0.8 * sample_cnt)\n",
    "\n",
    "train_sample = df.iloc[:train_eval_split, :]\n",
    "test_sample = df.iloc[train_eval_split:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_xy(df:pd.DataFrame):\n",
    "    label_col_flags = df.columns.str.find(\"labels\") == 0\n",
    "    assert sum(label_col_flags) == 1\n",
    "    x = df.loc[:, ~label_col_flags]\n",
    "    y = df.loc[:, label_col_flags]\n",
    "    return x, y.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: (222350, 11) (222350,)\n",
      "eval: (55588, 11) (55588,)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = split_xy(train_sample)\n",
    "print(\"training:\", train_x.shape, train_y.shape)\n",
    "eval_x, eval_y = split_xy(test_sample)\n",
    "print(\"eval:\", eval_x.shape, eval_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flybywindwen/miniconda3/lib/python3.10/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n",
      "  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2803\n",
      "[LightGBM] [Info] Number of data points in the train set: 222350, number of used features: 11\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score -1.213356\n",
      "[LightGBM] [Info] Start training from score -0.966460\n",
      "[LightGBM] [Info] Start training from score -1.738693\n",
      "[LightGBM] [Info] Start training from score -1.919879\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's multi_logloss: 1.29469\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's multi_logloss: 1.27402\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's multi_logloss: 1.25405\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's multi_logloss: 1.2348\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's multi_logloss: 1.21615\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's multi_logloss: 1.19808\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's multi_logloss: 1.18062\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's multi_logloss: 1.16369\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's multi_logloss: 1.14725\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's multi_logloss: 1.13127\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's multi_logloss: 1.11572\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's multi_logloss: 1.10062\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's multi_logloss: 1.08594\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's multi_logloss: 1.07162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's multi_logloss: 1.05762\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's multi_logloss: 1.04398\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's multi_logloss: 1.03065\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's multi_logloss: 1.01771\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's multi_logloss: 1.00505\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's multi_logloss: 0.992687\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[21]\tvalid_0's multi_logloss: 0.980651\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\tvalid_0's multi_logloss: 0.968837\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tvalid_0's multi_logloss: 0.95729\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\tvalid_0's multi_logloss: 0.946056\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[25]\tvalid_0's multi_logloss: 0.935038\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\tvalid_0's multi_logloss: 0.924281\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\tvalid_0's multi_logloss: 0.913775\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\tvalid_0's multi_logloss: 0.903452\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\tvalid_0's multi_logloss: 0.89328\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\tvalid_0's multi_logloss: 0.883417\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\tvalid_0's multi_logloss: 0.87358\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\tvalid_0's multi_logloss: 0.864042\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[33]\tvalid_0's multi_logloss: 0.854692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\tvalid_0's multi_logloss: 0.845573\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\tvalid_0's multi_logloss: 0.836661\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\tvalid_0's multi_logloss: 0.827857\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\tvalid_0's multi_logloss: 0.819093\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\tvalid_0's multi_logloss: 0.810615\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\tvalid_0's multi_logloss: 0.802361\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\tvalid_0's multi_logloss: 0.794248\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\tvalid_0's multi_logloss: 0.786256\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\tvalid_0's multi_logloss: 0.778387\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[43]\tvalid_0's multi_logloss: 0.770782\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\tvalid_0's multi_logloss: 0.763145\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\tvalid_0's multi_logloss: 0.755712\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\tvalid_0's multi_logloss: 0.748477\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\tvalid_0's multi_logloss: 0.741291\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\tvalid_0's multi_logloss: 0.734186\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[49]\tvalid_0's multi_logloss: 0.727243\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\tvalid_0's multi_logloss: 0.720377\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[51]\tvalid_0's multi_logloss: 0.713766\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[52]\tvalid_0's multi_logloss: 0.707121\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\tvalid_0's multi_logloss: 0.700703\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[54]\tvalid_0's multi_logloss: 0.694282\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[55]\tvalid_0's multi_logloss: 0.687973\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[56]\tvalid_0's multi_logloss: 0.681885\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[57]\tvalid_0's multi_logloss: 0.67591\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[58]\tvalid_0's multi_logloss: 0.669947\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[59]\tvalid_0's multi_logloss: 0.664033\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[60]\tvalid_0's multi_logloss: 0.658375\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[61]\tvalid_0's multi_logloss: 0.652699\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[62]\tvalid_0's multi_logloss: 0.64721\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[63]\tvalid_0's multi_logloss: 0.641818\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[64]\tvalid_0's multi_logloss: 0.636315\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[65]\tvalid_0's multi_logloss: 0.631103\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[66]\tvalid_0's multi_logloss: 0.625853\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[67]\tvalid_0's multi_logloss: 0.620638\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[68]\tvalid_0's multi_logloss: 0.615664\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[69]\tvalid_0's multi_logloss: 0.610614\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[70]\tvalid_0's multi_logloss: 0.605785\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[71]\tvalid_0's multi_logloss: 0.600931\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[72]\tvalid_0's multi_logloss: 0.596122\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[73]\tvalid_0's multi_logloss: 0.59154\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[74]\tvalid_0's multi_logloss: 0.586864\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[75]\tvalid_0's multi_logloss: 0.582329\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[76]\tvalid_0's multi_logloss: 0.577924\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[77]\tvalid_0's multi_logloss: 0.573518\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[78]\tvalid_0's multi_logloss: 0.569138\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[79]\tvalid_0's multi_logloss: 0.564943\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[80]\tvalid_0's multi_logloss: 0.560681\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[81]\tvalid_0's multi_logloss: 0.556536\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[82]\tvalid_0's multi_logloss: 0.55241\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[83]\tvalid_0's multi_logloss: 0.54845\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[84]\tvalid_0's multi_logloss: 0.544581\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[85]\tvalid_0's multi_logloss: 0.540672\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[86]\tvalid_0's multi_logloss: 0.536809\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[87]\tvalid_0's multi_logloss: 0.533018\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[88]\tvalid_0's multi_logloss: 0.529273\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[89]\tvalid_0's multi_logloss: 0.52555\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[90]\tvalid_0's multi_logloss: 0.521996\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[91]\tvalid_0's multi_logloss: 0.518429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[92]\tvalid_0's multi_logloss: 0.514922\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[93]\tvalid_0's multi_logloss: 0.511421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[94]\tvalid_0's multi_logloss: 0.507976\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[95]\tvalid_0's multi_logloss: 0.504576\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[96]\tvalid_0's multi_logloss: 0.501249\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[97]\tvalid_0's multi_logloss: 0.497892\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[98]\tvalid_0's multi_logloss: 0.494677\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[99]\tvalid_0's multi_logloss: 0.491427\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's multi_logloss: 0.488209\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(learning_rate=0.01, max_depth=5, min_child_samples=100,\n",
       "               min_split_gain=0.1, num_leaves=47, objective=&#x27;multiclass&#x27;,\n",
       "               silent=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(learning_rate=0.01, max_depth=5, min_child_samples=100,\n",
       "               min_split_gain=0.1, num_leaves=47, objective=&#x27;multiclass&#x27;,\n",
       "               silent=False)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(learning_rate=0.01, max_depth=5, min_child_samples=100,\n",
       "               min_split_gain=0.1, num_leaves=47, objective='multiclass',\n",
       "               silent=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_classifier = lgbm.LGBMClassifier(max_depth=5, learning_rate=0.01, min_split_gain=0.1, min_child_samples=100, num_leaves=47, objective='multiclass', silent=False)\n",
    "lgbm_classifier.fit(train_x, train_y, eval_set=(eval_x, eval_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = lgbm_classifier.predict(eval_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    24684\n",
       "0    15342\n",
       "3     9646\n",
       "2     5916\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(pred_y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "1    21841\n",
       "0    15976\n",
       "3     9784\n",
       "2     7987\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default loss: 0.8935741526948262\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"default loss:\", 1-sklearn_metric_loss_score(\"accuracy\", pred_y, eval_y))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLAML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 06-28 22:27:42] {1693} INFO - task = multiclass\n",
      "[flaml.automl.logger: 06-28 22:27:42] {1700} INFO - Data split method: stratified\n",
      "[flaml.automl.logger: 06-28 22:27:42] {1703} INFO - Evaluation method: holdout\n",
      "[flaml.automl.logger: 06-28 22:27:43] {1801} INFO - Minimizing error metric: 1-accuracy\n",
      "[flaml.automl.logger: 06-28 22:27:43] {1911} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
      "[flaml.automl.logger: 06-28 22:27:43] {2221} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:27:43] {2347} INFO - Estimated sufficient time budget=6222s. Estimated necessary time budget=6s.\n",
      "[flaml.automl.logger: 06-28 22:27:43] {2394} INFO -  at 0.5s,\testimator lgbm's best error=0.2242,\tbest estimator lgbm's best error=0.2242\n",
      "[flaml.automl.logger: 06-28 22:27:43] {2221} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:27:43] {2394} INFO -  at 0.5s,\testimator lgbm's best error=0.2242,\tbest estimator lgbm's best error=0.2242\n",
      "[flaml.automl.logger: 06-28 22:27:43] {2221} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:27:43] {2394} INFO -  at 0.6s,\testimator lgbm's best error=0.0991,\tbest estimator lgbm's best error=0.0991\n",
      "[flaml.automl.logger: 06-28 22:27:43] {2221} INFO - iteration 3, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:27:43] {2394} INFO -  at 1.1s,\testimator lgbm's best error=0.0671,\tbest estimator lgbm's best error=0.0671\n",
      "[flaml.automl.logger: 06-28 22:27:43] {2221} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:27:45] {2394} INFO -  at 2.6s,\testimator lgbm's best error=0.0671,\tbest estimator lgbm's best error=0.0671\n",
      "[flaml.automl.logger: 06-28 22:27:45] {2221} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:27:45] {2394} INFO -  at 2.6s,\testimator lgbm's best error=0.0671,\tbest estimator lgbm's best error=0.0671\n",
      "[flaml.automl.logger: 06-28 22:27:45] {2221} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:27:45] {2394} INFO -  at 3.0s,\testimator lgbm's best error=0.0671,\tbest estimator lgbm's best error=0.0671\n",
      "[flaml.automl.logger: 06-28 22:27:45] {2221} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:27:46] {2394} INFO -  at 3.6s,\testimator lgbm's best error=0.0523,\tbest estimator lgbm's best error=0.0523\n",
      "[flaml.automl.logger: 06-28 22:27:46] {2221} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:27:46] {2394} INFO -  at 3.7s,\testimator lgbm's best error=0.0523,\tbest estimator lgbm's best error=0.0523\n",
      "[flaml.automl.logger: 06-28 22:27:46] {2221} INFO - iteration 9, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:27:49] {2394} INFO -  at 6.7s,\testimator lgbm's best error=0.0500,\tbest estimator lgbm's best error=0.0500\n",
      "[flaml.automl.logger: 06-28 22:27:49] {2221} INFO - iteration 10, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:27:50] {2394} INFO -  at 7.5s,\testimator lgbm's best error=0.0500,\tbest estimator lgbm's best error=0.0500\n",
      "[flaml.automl.logger: 06-28 22:27:50] {2221} INFO - iteration 11, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:27:55] {2394} INFO -  at 12.7s,\testimator lgbm's best error=0.0500,\tbest estimator lgbm's best error=0.0500\n",
      "[flaml.automl.logger: 06-28 22:27:55] {2221} INFO - iteration 12, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:27:56] {2394} INFO -  at 13.6s,\testimator lgbm's best error=0.0500,\tbest estimator lgbm's best error=0.0500\n",
      "[flaml.automl.logger: 06-28 22:27:56] {2221} INFO - iteration 13, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:28:00] {2394} INFO -  at 18.3s,\testimator lgbm's best error=0.0500,\tbest estimator lgbm's best error=0.0500\n",
      "[flaml.automl.logger: 06-28 22:28:00] {2221} INFO - iteration 14, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:28:03] {2394} INFO -  at 20.7s,\testimator lgbm's best error=0.0500,\tbest estimator lgbm's best error=0.0500\n",
      "[flaml.automl.logger: 06-28 22:28:03] {2221} INFO - iteration 15, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:28:07] {2394} INFO -  at 25.2s,\testimator lgbm's best error=0.0331,\tbest estimator lgbm's best error=0.0331\n",
      "[flaml.automl.logger: 06-28 22:28:07] {2221} INFO - iteration 16, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:28:42] {2394} INFO -  at 60.2s,\testimator lgbm's best error=0.0331,\tbest estimator lgbm's best error=0.0331\n",
      "[flaml.automl.logger: 06-28 22:28:47] {2630} INFO - retrain lgbm for 4.7s\n",
      "[flaml.automl.logger: 06-28 22:28:47] {2633} INFO - retrained model: LGBMClassifier(learning_rate=0.2647748120311755, max_bin=1023,\n",
      "               min_child_samples=29, n_estimators=56, num_leaves=106,\n",
      "               reg_alpha=0.0009765625, reg_lambda=0.028486834222229064,\n",
      "               verbose=-1)\n",
      "[flaml.automl.logger: 06-28 22:28:47] {1941} INFO - fit succeeded\n",
      "[flaml.automl.logger: 06-28 22:28:47] {1942} INFO - Time taken to find the best model: 25.234309196472168\n"
     ]
    }
   ],
   "source": [
    "lgbm_flaml = AutoML()\n",
    "settings = {\n",
    "    \"time_budget\": 60,  # total running time in seconds\n",
    "    \"metric\": 'accuracy',  # primary metrics for regression can be chosen from: ['mae','mse','r2','rmse','mape']\n",
    "    \"estimator_list\": ['lgbm'],  # list of ML learners; we tune lightgbm in this example\n",
    "    \"task\": 'multiclass',  # task type    \n",
    "    \"log_file_name\": 'spotify_song_catigory_flaml.log',  # flaml log file\n",
    "    \"seed\": 7654321,    # random seed\n",
    "}\n",
    "lgbm_flaml.fit(X_train=train_x, y_train=train_y, **settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 06-28 22:31:10] {1693} INFO - task = multiclass\n",
      "[flaml.automl.logger: 06-28 22:31:10] {1700} INFO - Data split method: stratified\n",
      "[flaml.automl.logger: 06-28 22:31:10] {1703} INFO - Evaluation method: holdout\n",
      "[flaml.automl.logger: 06-28 22:31:10] {1801} INFO - Minimizing error metric: 1-accuracy\n",
      "[flaml.automl.logger: 06-28 22:31:10] {1911} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
      "[flaml.automl.logger: 06-28 22:31:10] {2221} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:31:10] {2347} INFO - Estimated sufficient time budget=7108s. Estimated necessary time budget=7s.\n",
      "[flaml.automl.logger: 06-28 22:31:10] {2394} INFO -  at 0.4s,\testimator lgbm's best error=0.2117,\tbest estimator lgbm's best error=0.2117\n",
      "[flaml.automl.logger: 06-28 22:31:10] {2221} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:31:10] {2394} INFO -  at 0.4s,\testimator lgbm's best error=0.2117,\tbest estimator lgbm's best error=0.2117\n",
      "[flaml.automl.logger: 06-28 22:31:10] {2221} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:31:10] {2394} INFO -  at 0.5s,\testimator lgbm's best error=0.0971,\tbest estimator lgbm's best error=0.0971\n",
      "[flaml.automl.logger: 06-28 22:31:10] {2221} INFO - iteration 3, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:31:10] {2394} INFO -  at 1.0s,\testimator lgbm's best error=0.0638,\tbest estimator lgbm's best error=0.0638\n",
      "[flaml.automl.logger: 06-28 22:31:10] {2221} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:31:12] {2394} INFO -  at 2.3s,\testimator lgbm's best error=0.0638,\tbest estimator lgbm's best error=0.0638\n",
      "[flaml.automl.logger: 06-28 22:31:12] {2221} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:31:12] {2394} INFO -  at 2.4s,\testimator lgbm's best error=0.0638,\tbest estimator lgbm's best error=0.0638\n",
      "[flaml.automl.logger: 06-28 22:31:12] {2221} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:31:12] {2394} INFO -  at 2.8s,\testimator lgbm's best error=0.0638,\tbest estimator lgbm's best error=0.0638\n",
      "[flaml.automl.logger: 06-28 22:31:12] {2221} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:31:13] {2394} INFO -  at 3.6s,\testimator lgbm's best error=0.0483,\tbest estimator lgbm's best error=0.0483\n",
      "[flaml.automl.logger: 06-28 22:31:13] {2221} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:31:13] {2394} INFO -  at 3.7s,\testimator lgbm's best error=0.0483,\tbest estimator lgbm's best error=0.0483\n",
      "[flaml.automl.logger: 06-28 22:31:13] {2221} INFO - iteration 9, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:31:16] {2394} INFO -  at 7.1s,\testimator lgbm's best error=0.0460,\tbest estimator lgbm's best error=0.0460\n",
      "[flaml.automl.logger: 06-28 22:31:16] {2221} INFO - iteration 10, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:31:17] {2394} INFO -  at 7.9s,\testimator lgbm's best error=0.0460,\tbest estimator lgbm's best error=0.0460\n",
      "[flaml.automl.logger: 06-28 22:31:17] {2221} INFO - iteration 11, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:31:22] {2394} INFO -  at 12.7s,\testimator lgbm's best error=0.0460,\tbest estimator lgbm's best error=0.0460\n",
      "[flaml.automl.logger: 06-28 22:31:22] {2221} INFO - iteration 12, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:31:23] {2394} INFO -  at 13.6s,\testimator lgbm's best error=0.0460,\tbest estimator lgbm's best error=0.0460\n",
      "[flaml.automl.logger: 06-28 22:31:23] {2221} INFO - iteration 13, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:31:27] {2394} INFO -  at 17.8s,\testimator lgbm's best error=0.0460,\tbest estimator lgbm's best error=0.0460\n",
      "[flaml.automl.logger: 06-28 22:31:27] {2221} INFO - iteration 14, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:31:31] {2394} INFO -  at 21.9s,\testimator lgbm's best error=0.0460,\tbest estimator lgbm's best error=0.0460\n",
      "[flaml.automl.logger: 06-28 22:31:31] {2221} INFO - iteration 15, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:31:36] {2394} INFO -  at 26.8s,\testimator lgbm's best error=0.0304,\tbest estimator lgbm's best error=0.0304\n",
      "[flaml.automl.logger: 06-28 22:31:36] {2221} INFO - iteration 16, current learner lgbm\n",
      "[flaml.automl.logger: 06-28 22:32:10] {2394} INFO -  at 60.6s,\testimator lgbm's best error=0.0304,\tbest estimator lgbm's best error=0.0304\n",
      "[flaml.automl.logger: 06-28 22:32:10] {2496} INFO - selected model: LGBMClassifier(learning_rate=0.2647748120311755, max_bin=1023,\n",
      "               min_child_samples=29, n_estimators=56, num_leaves=106,\n",
      "               reg_alpha=0.0009765625, reg_lambda=0.028486834222229064,\n",
      "               verbose=-1)\n",
      "[flaml.automl.logger: 06-28 22:32:10] {1941} INFO - fit succeeded\n",
      "[flaml.automl.logger: 06-28 22:32:10] {1942} INFO - Time taken to find the best model: 26.75779891014099\n"
     ]
    }
   ],
   "source": [
    "start_point = lgbm_flaml.best_config\n",
    "lgbm_flaml2 = AutoML()\n",
    "settings = {\n",
    "    \"time_budget\": 60,  # total running time in seconds\n",
    "    \"metric\": 'accuracy',  # primary metrics for regression can be chosen from: ['mae','mse','r2','rmse','mape']\n",
    "    \"estimator_list\": ['lgbm'],  # list of ML learners; we tune lightgbm in this example\n",
    "    \"task\": 'multiclass',  # task type    \n",
    "    \"log_file_name\": 'spotify_song_catigory_flaml.log',  # flaml log file\n",
    "    \"seed\": 7654321,    # random seed\n",
    "}\n",
    "lgbm_flaml2.fit(X_train=train_x, y_train=train_y, starting_points=start_point, X_val=eval_x, y_val=eval_y, **settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flaml loss: 0.9696337338994028\n"
     ]
    }
   ],
   "source": [
    "pred_y = lgbm_flaml2.predict(eval_x)\n",
    "print(\"flaml loss:\", 1-sklearn_metric_loss_score(\"accuracy\", pred_y, eval_y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.integration.lightgbm as lgb\n",
    "dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "dval = lgb.Dataset(eval_x, label=eval_y)\n",
    "params = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"metric\": \"logloss\",\n",
    "    \"verbosity\": -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-28 22:42:10,323] A new study created in memory with name: no-name-8bc3c9ea-1943-400a-bc50-bd5c99ca4068\n",
      "feature_fraction, val_score: inf:   0%|          | 0/7 [00:00<?, ?it/s]/Users/flybywindwen/miniconda3/lib/python3.10/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "[LightGBM] [Fatal] Number of classes should be specified and greater than 1 for multiclass training\n",
      "[W 2023-06-28 22:42:10,380] Trial 0 failed with parameters: {'feature_fraction': 0.6} because of the following error: LightGBMError('Number of classes should be specified and greater than 1 for multiclass training').\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/flybywindwen/miniconda3/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/Users/flybywindwen/miniconda3/lib/python3.10/site-packages/optuna/integration/_lightgbm_tuner/optimize.py\", line 240, in __call__\n",
      "    booster = lgb.train(self.lgbm_params, train_set, **kwargs)\n",
      "  File \"/Users/flybywindwen/miniconda3/lib/python3.10/site-packages/lightgbm/engine.py\", line 271, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"/Users/flybywindwen/miniconda3/lib/python3.10/site-packages/lightgbm/basic.py\", line 2605, in __init__\n",
      "    train_set.construct()\n",
      "  File \"/Users/flybywindwen/miniconda3/lib/python3.10/site-packages/lightgbm/basic.py\", line 1815, in construct\n",
      "    self._lazy_init(self.data, label=self.label,\n",
      "  File \"/Users/flybywindwen/miniconda3/lib/python3.10/site-packages/lightgbm/basic.py\", line 1538, in _lazy_init\n",
      "    self.__init_from_np2d(data, params_str, ref_dataset)\n",
      "  File \"/Users/flybywindwen/miniconda3/lib/python3.10/site-packages/lightgbm/basic.py\", line 1659, in __init_from_np2d\n",
      "    _safe_call(_LIB.LGBM_DatasetCreateFromMat(\n",
      "  File \"/Users/flybywindwen/miniconda3/lib/python3.10/site-packages/lightgbm/basic.py\", line 125, in _safe_call\n",
      "    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))\n",
      "lightgbm.basic.LightGBMError: Number of classes should be specified and greater than 1 for multiclass training\n",
      "[W 2023-06-28 22:42:10,384] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Number of classes should be specified and greater than 1 for multiclass training",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mtrain(params, dtrain, valid_sets\u001b[39m=\u001b[39;49m[dval], verbose_eval\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m)        \n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/optuna/integration/_lightgbm_tuner/__init__.py:37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m _imports\u001b[39m.\u001b[39mcheck()\n\u001b[1;32m     36\u001b[0m auto_booster \u001b[39m=\u001b[39m LightGBMTuner(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 37\u001b[0m auto_booster\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m     38\u001b[0m \u001b[39mreturn\u001b[39;00m auto_booster\u001b[39m.\u001b[39mget_best_booster()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/optuna/integration/_lightgbm_tuner/optimize.py:536\u001b[0m, in \u001b[0;36m_LightGBMBaseTuner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[39m# Sampling.\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_train_set()\n\u001b[0;32m--> 536\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtune_feature_fraction()\n\u001b[1;32m    537\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtune_num_leaves()\n\u001b[1;32m    538\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtune_bagging()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/optuna/integration/_lightgbm_tuner/optimize.py:561\u001b[0m, in \u001b[0;36m_LightGBMBaseTuner.tune_feature_fraction\u001b[0;34m(self, n_trials)\u001b[0m\n\u001b[1;32m    558\u001b[0m param_values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinspace(\u001b[39m0.4\u001b[39m, \u001b[39m1.0\u001b[39m, n_trials)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    560\u001b[0m sampler \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39msamplers\u001b[39m.\u001b[39mGridSampler({param_name: param_values})\n\u001b[0;32m--> 561\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tune_params([param_name], \u001b[39mlen\u001b[39;49m(param_values), sampler, \u001b[39m\"\u001b[39;49m\u001b[39mfeature_fraction\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/optuna/integration/_lightgbm_tuner/optimize.py:644\u001b[0m, in \u001b[0;36m_LightGBMBaseTuner._tune_params\u001b[0;34m(self, target_param_names, n_trials, sampler, step_name)\u001b[0m\n\u001b[1;32m    642\u001b[0m     _timeout \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[39mif\u001b[39;00m _n_trials \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 644\u001b[0m     study\u001b[39m.\u001b[39;49moptimize(\n\u001b[1;32m    645\u001b[0m         objective,\n\u001b[1;32m    646\u001b[0m         n_trials\u001b[39m=\u001b[39;49m_n_trials,\n\u001b[1;32m    647\u001b[0m         timeout\u001b[39m=\u001b[39;49m_timeout,\n\u001b[1;32m    648\u001b[0m         catch\u001b[39m=\u001b[39;49m(),\n\u001b[1;32m    649\u001b[0m         callbacks\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optuna_callbacks,\n\u001b[1;32m    650\u001b[0m     )\n\u001b[1;32m    652\u001b[0m \u001b[39mif\u001b[39;00m pbar:\n\u001b[1;32m    653\u001b[0m     pbar\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/optuna/study/study.py:443\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     _optimize(\n\u001b[1;32m    444\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    445\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    446\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    447\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    448\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    449\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    450\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    451\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    452\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    453\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/optuna/integration/_lightgbm_tuner/optimize.py:240\u001b[0m, in \u001b[0;36m_OptunaObjective.__call__\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    238\u001b[0m kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlgbm_kwargs)\n\u001b[1;32m    239\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mvalid_sets\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copy_valid_sets(kwargs[\u001b[39m\"\u001b[39m\u001b[39mvalid_sets\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 240\u001b[0m booster \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mtrain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlgbm_params, train_set, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    242\u001b[0m val_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_booster_best_score(booster)\n\u001b[1;32m    243\u001b[0m elapsed_secs \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/lightgbm/engine.py:271\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39m# construct booster\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     booster \u001b[39m=\u001b[39m Booster(params\u001b[39m=\u001b[39;49mparams, train_set\u001b[39m=\u001b[39;49mtrain_set)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m is_valid_contain_train:\n\u001b[1;32m    273\u001b[0m         booster\u001b[39m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/lightgbm/basic.py:2605\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[0;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[1;32m   2598\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_network(\n\u001b[1;32m   2599\u001b[0m         machines\u001b[39m=\u001b[39mmachines,\n\u001b[1;32m   2600\u001b[0m         local_listen_port\u001b[39m=\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mlocal_listen_port\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   2601\u001b[0m         listen_time_out\u001b[39m=\u001b[39mparams\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtime_out\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m120\u001b[39m),\n\u001b[1;32m   2602\u001b[0m         num_machines\u001b[39m=\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mnum_machines\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   2603\u001b[0m     )\n\u001b[1;32m   2604\u001b[0m \u001b[39m# construct booster object\u001b[39;00m\n\u001b[0;32m-> 2605\u001b[0m train_set\u001b[39m.\u001b[39;49mconstruct()\n\u001b[1;32m   2606\u001b[0m \u001b[39m# copy the parameters from train_set\u001b[39;00m\n\u001b[1;32m   2607\u001b[0m params\u001b[39m.\u001b[39mupdate(train_set\u001b[39m.\u001b[39mget_params())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/lightgbm/basic.py:1815\u001b[0m, in \u001b[0;36mDataset.construct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1812\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_init_score_by_predictor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predictor, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, used_indices)\n\u001b[1;32m   1813\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1814\u001b[0m     \u001b[39m# create train\u001b[39;00m\n\u001b[0;32m-> 1815\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lazy_init(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, label\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel,\n\u001b[1;32m   1816\u001b[0m                     weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, group\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroup,\n\u001b[1;32m   1817\u001b[0m                     init_score\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_score, predictor\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predictor,\n\u001b[1;32m   1818\u001b[0m                     silent\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msilent, feature_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_name,\n\u001b[1;32m   1819\u001b[0m                     categorical_feature\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcategorical_feature, params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m   1820\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfree_raw_data:\n\u001b[1;32m   1821\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/lightgbm/basic.py:1538\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[0;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__init_from_csc(data, params_str, ref_dataset)\n\u001b[1;32m   1537\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m-> 1538\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__init_from_np2d(data, params_str, ref_dataset)\n\u001b[1;32m   1539\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1540\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(x, np\u001b[39m.\u001b[39mndarray) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/lightgbm/basic.py:1659\u001b[0m, in \u001b[0;36mDataset.__init_from_np2d\u001b[0;34m(self, mat, params_str, ref_dataset)\u001b[0m\n\u001b[1;32m   1656\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(mat\u001b[39m.\u001b[39mreshape(mat\u001b[39m.\u001b[39msize), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m   1658\u001b[0m ptr_data, type_ptr_data, _ \u001b[39m=\u001b[39m c_float_array(data)\n\u001b[0;32m-> 1659\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_DatasetCreateFromMat(\n\u001b[1;32m   1660\u001b[0m     ptr_data,\n\u001b[1;32m   1661\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(type_ptr_data),\n\u001b[1;32m   1662\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int32(mat\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]),\n\u001b[1;32m   1663\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int32(mat\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]),\n\u001b[1;32m   1664\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(C_API_IS_ROW_MAJOR),\n\u001b[1;32m   1665\u001b[0m     c_str(params_str),\n\u001b[1;32m   1666\u001b[0m     ref_dataset,\n\u001b[1;32m   1667\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle)))\n\u001b[1;32m   1668\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/lightgbm/basic.py:125\u001b[0m, in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m    The return value from C API calls.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(_LIB\u001b[39m.\u001b[39mLGBM_GetLastError()\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mLightGBMError\u001b[0m: Number of classes should be specified and greater than 1 for multiclass training"
     ]
    }
   ],
   "source": [
    "model = lgb.train(params, dtrain, valid_sets=[dval], verbose_eval=10000)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
