{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgbm\n",
    "from flaml import AutoML\n",
    "from flaml.ml import sklearn_metric_loss_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration (ms)</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>spec_rate</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>195000.0</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-8.815</td>\n",
       "      <td>0.0672</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.7530</td>\n",
       "      <td>0.520</td>\n",
       "      <td>128.050</td>\n",
       "      <td>3.446154e-07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194641.0</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.781</td>\n",
       "      <td>-6.848</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.009530</td>\n",
       "      <td>0.3490</td>\n",
       "      <td>0.250</td>\n",
       "      <td>122.985</td>\n",
       "      <td>1.464234e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>217573.0</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-8.029</td>\n",
       "      <td>0.0872</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.247</td>\n",
       "      <td>170.044</td>\n",
       "      <td>4.007850e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>443478.0</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.699</td>\n",
       "      <td>-4.571</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.0888</td>\n",
       "      <td>0.199</td>\n",
       "      <td>92.011</td>\n",
       "      <td>7.959809e-08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>225862.0</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.771</td>\n",
       "      <td>-5.863</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.3650</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0965</td>\n",
       "      <td>0.163</td>\n",
       "      <td>115.917</td>\n",
       "      <td>4.693131e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration (ms)  danceability  energy  loudness  speechiness  acousticness  \\\n",
       "0       195000.0         0.611   0.614    -8.815       0.0672        0.0169   \n",
       "1       194641.0         0.638   0.781    -6.848       0.0285        0.0118   \n",
       "2       217573.0         0.560   0.810    -8.029       0.0872        0.0071   \n",
       "3       443478.0         0.525   0.699    -4.571       0.0353        0.0178   \n",
       "4       225862.0         0.367   0.771    -5.863       0.1060        0.3650   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo     spec_rate  labels  \n",
       "0          0.000794    0.7530    0.520  128.050  3.446154e-07       2  \n",
       "1          0.009530    0.3490    0.250  122.985  1.464234e-07       1  \n",
       "2          0.000008    0.2410    0.247  170.044  4.007850e-07       1  \n",
       "3          0.000088    0.0888    0.199   92.011  7.959809e-08       0  \n",
       "4          0.000001    0.0965    0.163  115.917  4.693131e-07       1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"278k_song_labelled.csv\")\n",
    "df = df.loc[:, ~df.columns.str.contains(\"unnamed\", case=False)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration (ms)</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>spec_rate</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.779380e+05</td>\n",
       "      <td>277938.000000</td>\n",
       "      <td>277938.000000</td>\n",
       "      <td>277938.000000</td>\n",
       "      <td>277938.000000</td>\n",
       "      <td>277938.000000</td>\n",
       "      <td>277938.000000</td>\n",
       "      <td>277938.000000</td>\n",
       "      <td>277938.000000</td>\n",
       "      <td>277938.000000</td>\n",
       "      <td>2.779380e+05</td>\n",
       "      <td>277938.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.324961e+05</td>\n",
       "      <td>0.552583</td>\n",
       "      <td>0.556866</td>\n",
       "      <td>-10.363654</td>\n",
       "      <td>0.087913</td>\n",
       "      <td>0.386583</td>\n",
       "      <td>0.255044</td>\n",
       "      <td>0.189217</td>\n",
       "      <td>0.449602</td>\n",
       "      <td>119.196002</td>\n",
       "      <td>4.754654e-07</td>\n",
       "      <td>1.179101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.171830e+05</td>\n",
       "      <td>0.188905</td>\n",
       "      <td>0.279681</td>\n",
       "      <td>6.672049</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.364504</td>\n",
       "      <td>0.373745</td>\n",
       "      <td>0.163596</td>\n",
       "      <td>0.267471</td>\n",
       "      <td>30.462256</td>\n",
       "      <td>9.190229e-07</td>\n",
       "      <td>1.021033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.706000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.720130e+05</td>\n",
       "      <td>0.431000</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>-12.747000</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096200</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>95.072250</td>\n",
       "      <td>1.531461e-07</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.131055e+05</td>\n",
       "      <td>0.571000</td>\n",
       "      <td>0.591000</td>\n",
       "      <td>-8.397000</td>\n",
       "      <td>0.047100</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>0.434000</td>\n",
       "      <td>119.940000</td>\n",
       "      <td>2.345459e-07</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.648660e+05</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>-5.842000</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.754000</td>\n",
       "      <td>0.645000</td>\n",
       "      <td>0.227000</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>138.869750</td>\n",
       "      <td>4.449937e-07</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.919895e+06</td>\n",
       "      <td>0.989000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.882000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>244.947000</td>\n",
       "      <td>5.971860e-05</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       duration (ms)   danceability         energy       loudness  \\\n",
       "count   2.779380e+05  277938.000000  277938.000000  277938.000000   \n",
       "mean    2.324961e+05       0.552583       0.556866     -10.363654   \n",
       "std     1.171830e+05       0.188905       0.279681       6.672049   \n",
       "min     6.706000e+03       0.000000       0.000000     -60.000000   \n",
       "25%     1.720130e+05       0.431000       0.342000     -12.747000   \n",
       "50%     2.131055e+05       0.571000       0.591000      -8.397000   \n",
       "75%     2.648660e+05       0.693000       0.792000      -5.842000   \n",
       "max     3.919895e+06       0.989000       1.000000       4.882000   \n",
       "\n",
       "         speechiness   acousticness  instrumentalness       liveness  \\\n",
       "count  277938.000000  277938.000000     277938.000000  277938.000000   \n",
       "mean        0.087913       0.386583          0.255044       0.189217   \n",
       "std         0.112500       0.364504          0.373745       0.163596   \n",
       "min         0.000000       0.000000          0.000000       0.000000   \n",
       "25%         0.035900       0.033800          0.000000       0.096200   \n",
       "50%         0.047100       0.262000          0.001090       0.121000   \n",
       "75%         0.082200       0.754000          0.645000       0.227000   \n",
       "max         0.965000       0.996000          1.000000       1.000000   \n",
       "\n",
       "             valence          tempo     spec_rate         labels  \n",
       "count  277938.000000  277938.000000  2.779380e+05  277938.000000  \n",
       "mean        0.449602     119.196002  4.754654e-07       1.179101  \n",
       "std         0.267471      30.462256  9.190229e-07       1.021033  \n",
       "min         0.000000       0.000000  0.000000e+00       0.000000  \n",
       "25%         0.220000      95.072250  1.531461e-07       0.000000  \n",
       "50%         0.434000     119.940000  2.345459e-07       1.000000  \n",
       "75%         0.665000     138.869750  4.449937e-07       2.000000  \n",
       "max         1.000000     244.947000  5.971860e-05       3.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "1    106429\n",
       "0     82058\n",
       "2     47065\n",
       "3     42386\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cnt = df.shape[0]\n",
    "train_eval_split = int(0.8 * sample_cnt)\n",
    "\n",
    "train_sample = df.iloc[:train_eval_split, :]\n",
    "test_sample = df.iloc[train_eval_split:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_xy(df:pd.DataFrame):\n",
    "    label_col_flags = df.columns.str.find(\"labels\") == 0\n",
    "    assert sum(label_col_flags) == 1\n",
    "    x = df.loc[:, ~label_col_flags]\n",
    "    y = df.loc[:, label_col_flags]\n",
    "    return x, y.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: (222350, 11) (222350,)\n",
      "eval: (55588, 11) (55588,)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = split_xy(train_sample)\n",
    "print(\"training:\", train_x.shape, train_y.shape)\n",
    "eval_x, eval_y = split_xy(test_sample)\n",
    "print(\"eval:\", eval_x.shape, eval_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flybywindwen/miniconda3/lib/python3.10/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n",
      "  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000830 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2803\n",
      "[LightGBM] [Info] Number of data points in the train set: 222350, number of used features: 11\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score -1.213356\n",
      "[LightGBM] [Info] Start training from score -0.966460\n",
      "[LightGBM] [Info] Start training from score -1.738693\n",
      "[LightGBM] [Info] Start training from score -1.919879\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's multi_logloss: 1.29469\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's multi_logloss: 1.27402\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's multi_logloss: 1.25405\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's multi_logloss: 1.2348\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's multi_logloss: 1.21615\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's multi_logloss: 1.19808\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's multi_logloss: 1.18062\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's multi_logloss: 1.16369\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's multi_logloss: 1.14725\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's multi_logloss: 1.13127\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's multi_logloss: 1.11572\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's multi_logloss: 1.10062\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's multi_logloss: 1.08594\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's multi_logloss: 1.07162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's multi_logloss: 1.05762\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's multi_logloss: 1.04398\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's multi_logloss: 1.03065\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's multi_logloss: 1.01771\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's multi_logloss: 1.00505\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's multi_logloss: 0.992687\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[21]\tvalid_0's multi_logloss: 0.980651\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\tvalid_0's multi_logloss: 0.968837\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tvalid_0's multi_logloss: 0.95729\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\tvalid_0's multi_logloss: 0.946056\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[25]\tvalid_0's multi_logloss: 0.935038\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\tvalid_0's multi_logloss: 0.924281\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\tvalid_0's multi_logloss: 0.913775\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\tvalid_0's multi_logloss: 0.903452\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\tvalid_0's multi_logloss: 0.89328\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\tvalid_0's multi_logloss: 0.883417\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\tvalid_0's multi_logloss: 0.87358\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\tvalid_0's multi_logloss: 0.864042\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[33]\tvalid_0's multi_logloss: 0.854692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\tvalid_0's multi_logloss: 0.845573\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\tvalid_0's multi_logloss: 0.836661\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\tvalid_0's multi_logloss: 0.827857\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\tvalid_0's multi_logloss: 0.819093\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\tvalid_0's multi_logloss: 0.810615\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\tvalid_0's multi_logloss: 0.802361\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\tvalid_0's multi_logloss: 0.794248\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\tvalid_0's multi_logloss: 0.786256\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\tvalid_0's multi_logloss: 0.778387\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[43]\tvalid_0's multi_logloss: 0.770782\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\tvalid_0's multi_logloss: 0.763145\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\tvalid_0's multi_logloss: 0.755712\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\tvalid_0's multi_logloss: 0.748477\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\tvalid_0's multi_logloss: 0.741291\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\tvalid_0's multi_logloss: 0.734186\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[49]\tvalid_0's multi_logloss: 0.727243\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\tvalid_0's multi_logloss: 0.720377\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[51]\tvalid_0's multi_logloss: 0.713766\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[52]\tvalid_0's multi_logloss: 0.707121\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\tvalid_0's multi_logloss: 0.700703\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[54]\tvalid_0's multi_logloss: 0.694282\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[55]\tvalid_0's multi_logloss: 0.687973\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[56]\tvalid_0's multi_logloss: 0.681885\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[57]\tvalid_0's multi_logloss: 0.67591\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[58]\tvalid_0's multi_logloss: 0.669947\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[59]\tvalid_0's multi_logloss: 0.664033\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[60]\tvalid_0's multi_logloss: 0.658375\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[61]\tvalid_0's multi_logloss: 0.652699\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[62]\tvalid_0's multi_logloss: 0.64721\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[63]\tvalid_0's multi_logloss: 0.641818\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[64]\tvalid_0's multi_logloss: 0.636315\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[65]\tvalid_0's multi_logloss: 0.631103\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[66]\tvalid_0's multi_logloss: 0.625853\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[67]\tvalid_0's multi_logloss: 0.620638\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[68]\tvalid_0's multi_logloss: 0.615664\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[69]\tvalid_0's multi_logloss: 0.610614\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[70]\tvalid_0's multi_logloss: 0.605785\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[71]\tvalid_0's multi_logloss: 0.600931\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[72]\tvalid_0's multi_logloss: 0.596122\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[73]\tvalid_0's multi_logloss: 0.59154\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[74]\tvalid_0's multi_logloss: 0.586864\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[75]\tvalid_0's multi_logloss: 0.582329\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[76]\tvalid_0's multi_logloss: 0.577924\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[77]\tvalid_0's multi_logloss: 0.573518\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[78]\tvalid_0's multi_logloss: 0.569138\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[79]\tvalid_0's multi_logloss: 0.564943\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[80]\tvalid_0's multi_logloss: 0.560681\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[81]\tvalid_0's multi_logloss: 0.556536\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[82]\tvalid_0's multi_logloss: 0.55241\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[83]\tvalid_0's multi_logloss: 0.54845\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[84]\tvalid_0's multi_logloss: 0.544581\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[85]\tvalid_0's multi_logloss: 0.540672\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[86]\tvalid_0's multi_logloss: 0.536809\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[87]\tvalid_0's multi_logloss: 0.533018\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[88]\tvalid_0's multi_logloss: 0.529273\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[89]\tvalid_0's multi_logloss: 0.52555\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[90]\tvalid_0's multi_logloss: 0.521996\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[91]\tvalid_0's multi_logloss: 0.518429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[92]\tvalid_0's multi_logloss: 0.514922\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[93]\tvalid_0's multi_logloss: 0.511421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[94]\tvalid_0's multi_logloss: 0.507976\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[95]\tvalid_0's multi_logloss: 0.504576\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[96]\tvalid_0's multi_logloss: 0.501249\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[97]\tvalid_0's multi_logloss: 0.497892\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[98]\tvalid_0's multi_logloss: 0.494677\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[99]\tvalid_0's multi_logloss: 0.491427\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's multi_logloss: 0.488209\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(learning_rate=0.01, max_depth=5, min_child_samples=100,\n",
       "               min_split_gain=0.1, num_leaves=47, objective=&#x27;multiclass&#x27;,\n",
       "               silent=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(learning_rate=0.01, max_depth=5, min_child_samples=100,\n",
       "               min_split_gain=0.1, num_leaves=47, objective=&#x27;multiclass&#x27;,\n",
       "               silent=False)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(learning_rate=0.01, max_depth=5, min_child_samples=100,\n",
       "               min_split_gain=0.1, num_leaves=47, objective='multiclass',\n",
       "               silent=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_classifier = lgbm.LGBMClassifier(max_depth=5, learning_rate=0.01, min_split_gain=0.1, min_child_samples=100, num_leaves=47, objective='multiclass', silent=False)\n",
    "lgbm_classifier.fit(train_x, train_y, eval_set=(eval_x, eval_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = lgbm_classifier.predict(eval_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    24684\n",
       "0    15342\n",
       "3     9646\n",
       "2     5916\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(pred_y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "1    21841\n",
       "0    15976\n",
       "3     9784\n",
       "2     7987\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default loss: 0.8935741526948262\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"default loss:\", 1-sklearn_metric_loss_score(\"accuracy\", pred_y, eval_y))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLAML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 06-29 09:09:03] {1693} INFO - task = multiclass\n",
      "[flaml.automl.logger: 06-29 09:09:03] {1700} INFO - Data split method: stratified\n",
      "[flaml.automl.logger: 06-29 09:09:03] {1703} INFO - Evaluation method: holdout\n",
      "[flaml.automl.logger: 06-29 09:09:03] {1801} INFO - Minimizing error metric: 1-accuracy\n",
      "[flaml.automl.logger: 06-29 09:09:03] {1911} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
      "[flaml.automl.logger: 06-29 09:09:03] {2221} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:09:03] {2347} INFO - Estimated sufficient time budget=5463s. Estimated necessary time budget=5s.\n",
      "[flaml.automl.logger: 06-29 09:09:03] {2394} INFO -  at 0.4s,\testimator lgbm's best error=0.2242,\tbest estimator lgbm's best error=0.2242\n",
      "[flaml.automl.logger: 06-29 09:09:03] {2221} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:09:03] {2394} INFO -  at 0.4s,\testimator lgbm's best error=0.2242,\tbest estimator lgbm's best error=0.2242\n",
      "[flaml.automl.logger: 06-29 09:09:03] {2221} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:09:03] {2394} INFO -  at 0.5s,\testimator lgbm's best error=0.0991,\tbest estimator lgbm's best error=0.0991\n",
      "[flaml.automl.logger: 06-29 09:09:03] {2221} INFO - iteration 3, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:09:04] {2394} INFO -  at 0.9s,\testimator lgbm's best error=0.0671,\tbest estimator lgbm's best error=0.0671\n",
      "[flaml.automl.logger: 06-29 09:09:04] {2221} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:09:05] {2394} INFO -  at 2.4s,\testimator lgbm's best error=0.0671,\tbest estimator lgbm's best error=0.0671\n",
      "[flaml.automl.logger: 06-29 09:09:05] {2221} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:09:05] {2394} INFO -  at 2.5s,\testimator lgbm's best error=0.0671,\tbest estimator lgbm's best error=0.0671\n",
      "[flaml.automl.logger: 06-29 09:09:05] {2221} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:09:06] {2394} INFO -  at 3.1s,\testimator lgbm's best error=0.0523,\tbest estimator lgbm's best error=0.0523\n",
      "[flaml.automl.logger: 06-29 09:09:06] {2221} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:09:06] {2394} INFO -  at 3.6s,\testimator lgbm's best error=0.0523,\tbest estimator lgbm's best error=0.0523\n",
      "[flaml.automl.logger: 06-29 09:09:06] {2221} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:09:07] {2394} INFO -  at 4.3s,\testimator lgbm's best error=0.0523,\tbest estimator lgbm's best error=0.0523\n",
      "[flaml.automl.logger: 06-29 09:09:07] {2221} INFO - iteration 9, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:09:07] {2394} INFO -  at 4.4s,\testimator lgbm's best error=0.0523,\tbest estimator lgbm's best error=0.0523\n",
      "[flaml.automl.logger: 06-29 09:09:07] {2221} INFO - iteration 10, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:09:10] {2394} INFO -  at 7.4s,\testimator lgbm's best error=0.0500,\tbest estimator lgbm's best error=0.0500\n",
      "[flaml.automl.logger: 06-29 09:09:10] {2221} INFO - iteration 11, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:09:11] {2394} INFO -  at 8.1s,\testimator lgbm's best error=0.0500,\tbest estimator lgbm's best error=0.0500\n",
      "[flaml.automl.logger: 06-29 09:09:11] {2221} INFO - iteration 12, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:09:16] {2394} INFO -  at 13.3s,\testimator lgbm's best error=0.0500,\tbest estimator lgbm's best error=0.0500\n",
      "[flaml.automl.logger: 06-29 09:09:16] {2221} INFO - iteration 13, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:09:17] {2394} INFO -  at 14.2s,\testimator lgbm's best error=0.0500,\tbest estimator lgbm's best error=0.0500\n",
      "[flaml.automl.logger: 06-29 09:09:17] {2221} INFO - iteration 14, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:09:22] {2394} INFO -  at 18.8s,\testimator lgbm's best error=0.0500,\tbest estimator lgbm's best error=0.0500\n",
      "[flaml.automl.logger: 06-29 09:09:22] {2221} INFO - iteration 15, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:09:24] {2394} INFO -  at 21.2s,\testimator lgbm's best error=0.0500,\tbest estimator lgbm's best error=0.0500\n",
      "[flaml.automl.logger: 06-29 09:09:24] {2221} INFO - iteration 16, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:09:28] {2394} INFO -  at 25.6s,\testimator lgbm's best error=0.0331,\tbest estimator lgbm's best error=0.0331\n",
      "[flaml.automl.logger: 06-29 09:09:28] {2221} INFO - iteration 17, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:03] {2394} INFO -  at 60.2s,\testimator lgbm's best error=0.0331,\tbest estimator lgbm's best error=0.0331\n",
      "[flaml.automl.logger: 06-29 09:10:08] {2630} INFO - retrain lgbm for 4.6s\n",
      "[flaml.automl.logger: 06-29 09:10:08] {2633} INFO - retrained model: LGBMClassifier(learning_rate=0.2647748120311755, max_bin=1023,\n",
      "               min_child_samples=29, n_estimators=56, num_leaves=106,\n",
      "               reg_alpha=0.0009765625, reg_lambda=0.028486834222229064,\n",
      "               verbose=-1)\n",
      "[flaml.automl.logger: 06-29 09:10:08] {1941} INFO - fit succeeded\n",
      "[flaml.automl.logger: 06-29 09:10:08] {1942} INFO - Time taken to find the best model: 25.594363927841187\n"
     ]
    }
   ],
   "source": [
    "lgbm_flaml = AutoML()\n",
    "settings = {\n",
    "    \"time_budget\": 60,  # total running time in seconds\n",
    "    \"metric\": 'accuracy',  # primary metrics for regression can be chosen from: ['mae','mse','r2','rmse','mape']\n",
    "    \"estimator_list\": ['lgbm'],  # list of ML learners; we tune lightgbm in this example\n",
    "    \"task\": 'multiclass',  # task type    \n",
    "    \"log_file_name\": 'spotify_song_catigory_flaml.log',  # flaml log file\n",
    "    \"seed\": 7654321,    # random seed\n",
    "}\n",
    "lgbm_flaml.fit(X_train=train_x, y_train=train_y, **settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 06-29 09:10:08] {1693} INFO - task = multiclass\n",
      "[flaml.automl.logger: 06-29 09:10:08] {1700} INFO - Data split method: stratified\n",
      "[flaml.automl.logger: 06-29 09:10:08] {1703} INFO - Evaluation method: holdout\n",
      "[flaml.automl.logger: 06-29 09:10:08] {1801} INFO - Minimizing error metric: 1-accuracy\n",
      "[flaml.automl.logger: 06-29 09:10:08] {1911} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
      "[flaml.automl.logger: 06-29 09:10:08] {2221} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:08] {2347} INFO - Estimated sufficient time budget=5914s. Estimated necessary time budget=6s.\n",
      "[flaml.automl.logger: 06-29 09:10:08] {2394} INFO -  at 0.3s,\testimator lgbm's best error=0.2117,\tbest estimator lgbm's best error=0.2117\n",
      "[flaml.automl.logger: 06-29 09:10:08] {2221} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:08] {2394} INFO -  at 0.4s,\testimator lgbm's best error=0.2117,\tbest estimator lgbm's best error=0.2117\n",
      "[flaml.automl.logger: 06-29 09:10:08] {2221} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:08] {2394} INFO -  at 0.5s,\testimator lgbm's best error=0.0971,\tbest estimator lgbm's best error=0.0971\n",
      "[flaml.automl.logger: 06-29 09:10:08] {2221} INFO - iteration 3, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:08] {2394} INFO -  at 0.9s,\testimator lgbm's best error=0.0638,\tbest estimator lgbm's best error=0.0638\n",
      "[flaml.automl.logger: 06-29 09:10:08] {2221} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:10] {2394} INFO -  at 2.3s,\testimator lgbm's best error=0.0638,\tbest estimator lgbm's best error=0.0638\n",
      "[flaml.automl.logger: 06-29 09:10:10] {2221} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:10] {2394} INFO -  at 2.4s,\testimator lgbm's best error=0.0638,\tbest estimator lgbm's best error=0.0638\n",
      "[flaml.automl.logger: 06-29 09:10:10] {2221} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:10] {2394} INFO -  at 2.7s,\testimator lgbm's best error=0.0638,\tbest estimator lgbm's best error=0.0638\n",
      "[flaml.automl.logger: 06-29 09:10:10] {2221} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:11] {2394} INFO -  at 3.3s,\testimator lgbm's best error=0.0483,\tbest estimator lgbm's best error=0.0483\n",
      "[flaml.automl.logger: 06-29 09:10:11] {2221} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:11] {2394} INFO -  at 3.5s,\testimator lgbm's best error=0.0483,\tbest estimator lgbm's best error=0.0483\n",
      "[flaml.automl.logger: 06-29 09:10:11] {2221} INFO - iteration 9, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:14] {2394} INFO -  at 6.5s,\testimator lgbm's best error=0.0460,\tbest estimator lgbm's best error=0.0460\n",
      "[flaml.automl.logger: 06-29 09:10:14] {2221} INFO - iteration 10, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:15] {2394} INFO -  at 7.3s,\testimator lgbm's best error=0.0460,\tbest estimator lgbm's best error=0.0460\n",
      "[flaml.automl.logger: 06-29 09:10:15] {2221} INFO - iteration 11, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:20] {2394} INFO -  at 12.1s,\testimator lgbm's best error=0.0460,\tbest estimator lgbm's best error=0.0460\n",
      "[flaml.automl.logger: 06-29 09:10:20] {2221} INFO - iteration 12, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:21] {2394} INFO -  at 13.1s,\testimator lgbm's best error=0.0460,\tbest estimator lgbm's best error=0.0460\n",
      "[flaml.automl.logger: 06-29 09:10:21] {2221} INFO - iteration 13, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:25] {2394} INFO -  at 17.4s,\testimator lgbm's best error=0.0460,\tbest estimator lgbm's best error=0.0460\n",
      "[flaml.automl.logger: 06-29 09:10:25] {2221} INFO - iteration 14, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:29] {2394} INFO -  at 21.5s,\testimator lgbm's best error=0.0458,\tbest estimator lgbm's best error=0.0458\n",
      "[flaml.automl.logger: 06-29 09:10:29] {2221} INFO - iteration 15, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:32] {2394} INFO -  at 24.6s,\testimator lgbm's best error=0.0458,\tbest estimator lgbm's best error=0.0458\n",
      "[flaml.automl.logger: 06-29 09:10:32] {2221} INFO - iteration 16, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:35] {2394} INFO -  at 27.9s,\testimator lgbm's best error=0.0458,\tbest estimator lgbm's best error=0.0458\n",
      "[flaml.automl.logger: 06-29 09:10:35] {2221} INFO - iteration 17, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:37] {2394} INFO -  at 29.1s,\testimator lgbm's best error=0.0458,\tbest estimator lgbm's best error=0.0458\n",
      "[flaml.automl.logger: 06-29 09:10:37] {2221} INFO - iteration 18, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:40] {2394} INFO -  at 32.0s,\testimator lgbm's best error=0.0458,\tbest estimator lgbm's best error=0.0458\n",
      "[flaml.automl.logger: 06-29 09:10:40] {2221} INFO - iteration 19, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:42] {2394} INFO -  at 34.8s,\testimator lgbm's best error=0.0458,\tbest estimator lgbm's best error=0.0458\n",
      "[flaml.automl.logger: 06-29 09:10:42] {2221} INFO - iteration 20, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:45] {2394} INFO -  at 37.1s,\testimator lgbm's best error=0.0458,\tbest estimator lgbm's best error=0.0458\n",
      "[flaml.automl.logger: 06-29 09:10:45] {2221} INFO - iteration 21, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:10:47] {2394} INFO -  at 39.4s,\testimator lgbm's best error=0.0458,\tbest estimator lgbm's best error=0.0458\n",
      "[flaml.automl.logger: 06-29 09:10:47] {2221} INFO - iteration 22, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:11:02] {2394} INFO -  at 54.2s,\testimator lgbm's best error=0.0458,\tbest estimator lgbm's best error=0.0458\n",
      "[flaml.automl.logger: 06-29 09:11:02] {2221} INFO - iteration 23, current learner lgbm\n",
      "[flaml.automl.logger: 06-29 09:11:08] {2394} INFO -  at 60.0s,\testimator lgbm's best error=0.0429,\tbest estimator lgbm's best error=0.0429\n",
      "[flaml.automl.logger: 06-29 09:11:08] {2496} INFO - selected model: LGBMClassifier(learning_rate=0.7557849801382729, max_bin=1023,\n",
      "               min_child_samples=31, n_estimators=18, num_leaves=690,\n",
      "               reg_alpha=0.0009765625, reg_lambda=0.007756568049822362,\n",
      "               verbose=-1)\n",
      "[flaml.automl.logger: 06-29 09:11:08] {1941} INFO - fit succeeded\n",
      "[flaml.automl.logger: 06-29 09:11:08] {1942} INFO - Time taken to find the best model: 60.02354192733765\n",
      "[flaml.automl.logger: 06-29 09:11:08] {1952} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n"
     ]
    }
   ],
   "source": [
    "start_point = lgbm_flaml.best_config\n",
    "lgbm_flaml2 = AutoML()\n",
    "settings = {\n",
    "    \"time_budget\": 60,  # total running time in seconds\n",
    "    \"metric\": 'accuracy',  # primary metrics for regression can be chosen from: ['mae','mse','r2','rmse','mape']\n",
    "    \"estimator_list\": ['lgbm'],  # list of ML learners; we tune lightgbm in this example\n",
    "    \"task\": 'multiclass',  # task type    \n",
    "    \"log_file_name\": 'spotify_song_catigory_flaml.log',  # flaml log file\n",
    "    \"seed\": 7654321,    # random seed\n",
    "}\n",
    "lgbm_flaml2.fit(X_train=train_x, y_train=train_y, starting_points=start_point, X_val=eval_x, y_val=eval_y, **settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flaml loss: 0.957113045981147\n"
     ]
    }
   ],
   "source": [
    "pred_y = lgbm_flaml2.predict(eval_x)\n",
    "print(\"flaml loss:\", 1-sklearn_metric_loss_score(\"accuracy\", pred_y, eval_y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.trial import Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-29 09:16:13,780] A new study created in memory with name: no-name-33fe284d-e69c-402e-b83c-b6c1376954f0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new study created in memory with name: no-name-33fe284d-e69c-402e-b83c-b6c1376954f0\n",
      "A new study created in memory with name: no-name-33fe284d-e69c-402e-b83c-b6c1376954f0\n",
      "A new study created in memory with name: no-name-33fe284d-e69c-402e-b83c-b6c1376954f0\n",
      "A new study created in memory with name: no-name-33fe284d-e69c-402e-b83c-b6c1376954f0\n",
      "A new study created in memory with name: no-name-33fe284d-e69c-402e-b83c-b6c1376954f0\n",
      "A new study created in memory with name: no-name-33fe284d-e69c-402e-b83c-b6c1376954f0\n",
      "A new study created in memory with name: no-name-33fe284d-e69c-402e-b83c-b6c1376954f0\n",
      "A new study created in memory with name: no-name-33fe284d-e69c-402e-b83c-b6c1376954f0\n",
      "A new study created in memory with name: no-name-33fe284d-e69c-402e-b83c-b6c1376954f0\n",
      "A new study created in memory with name: no-name-33fe284d-e69c-402e-b83c-b6c1376954f0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-29 09:19:06,003] Trial 0 finished with value: 0.20646542419227176 and parameters: {'num_leaves': 731, 'learning_rate': 0.0012150794866611176, 'n_estimators': 353, 'subsample_for_bin': 19730, 'min_split_gain': 0.09177185212607926, 'min_child_weight': 0.001088091277809029, 'min_child_samples': 36, 'colsample_bytree': 0.378499385758178, 'reg_alpha': 0.18264034315092803, 'reg_lambda': 0.1344564735495806, 'boosting_type': 'dart'}. Best is trial 0 with value: 0.20646542419227176.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 finished with value: 0.20646542419227176 and parameters: {'num_leaves': 731, 'learning_rate': 0.0012150794866611176, 'n_estimators': 353, 'subsample_for_bin': 19730, 'min_split_gain': 0.09177185212607926, 'min_child_weight': 0.001088091277809029, 'min_child_samples': 36, 'colsample_bytree': 0.378499385758178, 'reg_alpha': 0.18264034315092803, 'reg_lambda': 0.1344564735495806, 'boosting_type': 'dart'}. Best is trial 0 with value: 0.20646542419227176.\n",
      "Trial 0 finished with value: 0.20646542419227176 and parameters: {'num_leaves': 731, 'learning_rate': 0.0012150794866611176, 'n_estimators': 353, 'subsample_for_bin': 19730, 'min_split_gain': 0.09177185212607926, 'min_child_weight': 0.001088091277809029, 'min_child_samples': 36, 'colsample_bytree': 0.378499385758178, 'reg_alpha': 0.18264034315092803, 'reg_lambda': 0.1344564735495806, 'boosting_type': 'dart'}. Best is trial 0 with value: 0.20646542419227176.\n",
      "Trial 0 finished with value: 0.20646542419227176 and parameters: {'num_leaves': 731, 'learning_rate': 0.0012150794866611176, 'n_estimators': 353, 'subsample_for_bin': 19730, 'min_split_gain': 0.09177185212607926, 'min_child_weight': 0.001088091277809029, 'min_child_samples': 36, 'colsample_bytree': 0.378499385758178, 'reg_alpha': 0.18264034315092803, 'reg_lambda': 0.1344564735495806, 'boosting_type': 'dart'}. Best is trial 0 with value: 0.20646542419227176.\n",
      "Trial 0 finished with value: 0.20646542419227176 and parameters: {'num_leaves': 731, 'learning_rate': 0.0012150794866611176, 'n_estimators': 353, 'subsample_for_bin': 19730, 'min_split_gain': 0.09177185212607926, 'min_child_weight': 0.001088091277809029, 'min_child_samples': 36, 'colsample_bytree': 0.378499385758178, 'reg_alpha': 0.18264034315092803, 'reg_lambda': 0.1344564735495806, 'boosting_type': 'dart'}. Best is trial 0 with value: 0.20646542419227176.\n",
      "Trial 0 finished with value: 0.20646542419227176 and parameters: {'num_leaves': 731, 'learning_rate': 0.0012150794866611176, 'n_estimators': 353, 'subsample_for_bin': 19730, 'min_split_gain': 0.09177185212607926, 'min_child_weight': 0.001088091277809029, 'min_child_samples': 36, 'colsample_bytree': 0.378499385758178, 'reg_alpha': 0.18264034315092803, 'reg_lambda': 0.1344564735495806, 'boosting_type': 'dart'}. Best is trial 0 with value: 0.20646542419227176.\n",
      "Trial 0 finished with value: 0.20646542419227176 and parameters: {'num_leaves': 731, 'learning_rate': 0.0012150794866611176, 'n_estimators': 353, 'subsample_for_bin': 19730, 'min_split_gain': 0.09177185212607926, 'min_child_weight': 0.001088091277809029, 'min_child_samples': 36, 'colsample_bytree': 0.378499385758178, 'reg_alpha': 0.18264034315092803, 'reg_lambda': 0.1344564735495806, 'boosting_type': 'dart'}. Best is trial 0 with value: 0.20646542419227176.\n",
      "Trial 0 finished with value: 0.20646542419227176 and parameters: {'num_leaves': 731, 'learning_rate': 0.0012150794866611176, 'n_estimators': 353, 'subsample_for_bin': 19730, 'min_split_gain': 0.09177185212607926, 'min_child_weight': 0.001088091277809029, 'min_child_samples': 36, 'colsample_bytree': 0.378499385758178, 'reg_alpha': 0.18264034315092803, 'reg_lambda': 0.1344564735495806, 'boosting_type': 'dart'}. Best is trial 0 with value: 0.20646542419227176.\n",
      "Trial 0 finished with value: 0.20646542419227176 and parameters: {'num_leaves': 731, 'learning_rate': 0.0012150794866611176, 'n_estimators': 353, 'subsample_for_bin': 19730, 'min_split_gain': 0.09177185212607926, 'min_child_weight': 0.001088091277809029, 'min_child_samples': 36, 'colsample_bytree': 0.378499385758178, 'reg_alpha': 0.18264034315092803, 'reg_lambda': 0.1344564735495806, 'boosting_type': 'dart'}. Best is trial 0 with value: 0.20646542419227176.\n",
      "Trial 0 finished with value: 0.20646542419227176 and parameters: {'num_leaves': 731, 'learning_rate': 0.0012150794866611176, 'n_estimators': 353, 'subsample_for_bin': 19730, 'min_split_gain': 0.09177185212607926, 'min_child_weight': 0.001088091277809029, 'min_child_samples': 36, 'colsample_bytree': 0.378499385758178, 'reg_alpha': 0.18264034315092803, 'reg_lambda': 0.1344564735495806, 'boosting_type': 'dart'}. Best is trial 0 with value: 0.20646542419227176.\n",
      "Trial 0 finished with value: 0.20646542419227176 and parameters: {'num_leaves': 731, 'learning_rate': 0.0012150794866611176, 'n_estimators': 353, 'subsample_for_bin': 19730, 'min_split_gain': 0.09177185212607926, 'min_child_weight': 0.001088091277809029, 'min_child_samples': 36, 'colsample_bytree': 0.378499385758178, 'reg_alpha': 0.18264034315092803, 'reg_lambda': 0.1344564735495806, 'boosting_type': 'dart'}. Best is trial 0 with value: 0.20646542419227176.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-29 09:19:47,837] Trial 1 finished with value: 0.030294308124055536 and parameters: {'num_leaves': 484, 'learning_rate': 0.05398174657392375, 'n_estimators': 245, 'subsample_for_bin': 33246, 'min_split_gain': 0.008247842164138808, 'min_child_weight': 0.00226070404942164, 'min_child_samples': 98, 'colsample_bytree': 0.6267341611507831, 'reg_alpha': 0.15822591691356322, 'reg_lambda': 0.19825096325778524, 'boosting_type': 'goss'}. Best is trial 1 with value: 0.030294308124055536.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 finished with value: 0.030294308124055536 and parameters: {'num_leaves': 484, 'learning_rate': 0.05398174657392375, 'n_estimators': 245, 'subsample_for_bin': 33246, 'min_split_gain': 0.008247842164138808, 'min_child_weight': 0.00226070404942164, 'min_child_samples': 98, 'colsample_bytree': 0.6267341611507831, 'reg_alpha': 0.15822591691356322, 'reg_lambda': 0.19825096325778524, 'boosting_type': 'goss'}. Best is trial 1 with value: 0.030294308124055536.\n",
      "Trial 1 finished with value: 0.030294308124055536 and parameters: {'num_leaves': 484, 'learning_rate': 0.05398174657392375, 'n_estimators': 245, 'subsample_for_bin': 33246, 'min_split_gain': 0.008247842164138808, 'min_child_weight': 0.00226070404942164, 'min_child_samples': 98, 'colsample_bytree': 0.6267341611507831, 'reg_alpha': 0.15822591691356322, 'reg_lambda': 0.19825096325778524, 'boosting_type': 'goss'}. Best is trial 1 with value: 0.030294308124055536.\n",
      "Trial 1 finished with value: 0.030294308124055536 and parameters: {'num_leaves': 484, 'learning_rate': 0.05398174657392375, 'n_estimators': 245, 'subsample_for_bin': 33246, 'min_split_gain': 0.008247842164138808, 'min_child_weight': 0.00226070404942164, 'min_child_samples': 98, 'colsample_bytree': 0.6267341611507831, 'reg_alpha': 0.15822591691356322, 'reg_lambda': 0.19825096325778524, 'boosting_type': 'goss'}. Best is trial 1 with value: 0.030294308124055536.\n",
      "Trial 1 finished with value: 0.030294308124055536 and parameters: {'num_leaves': 484, 'learning_rate': 0.05398174657392375, 'n_estimators': 245, 'subsample_for_bin': 33246, 'min_split_gain': 0.008247842164138808, 'min_child_weight': 0.00226070404942164, 'min_child_samples': 98, 'colsample_bytree': 0.6267341611507831, 'reg_alpha': 0.15822591691356322, 'reg_lambda': 0.19825096325778524, 'boosting_type': 'goss'}. Best is trial 1 with value: 0.030294308124055536.\n",
      "Trial 1 finished with value: 0.030294308124055536 and parameters: {'num_leaves': 484, 'learning_rate': 0.05398174657392375, 'n_estimators': 245, 'subsample_for_bin': 33246, 'min_split_gain': 0.008247842164138808, 'min_child_weight': 0.00226070404942164, 'min_child_samples': 98, 'colsample_bytree': 0.6267341611507831, 'reg_alpha': 0.15822591691356322, 'reg_lambda': 0.19825096325778524, 'boosting_type': 'goss'}. Best is trial 1 with value: 0.030294308124055536.\n",
      "Trial 1 finished with value: 0.030294308124055536 and parameters: {'num_leaves': 484, 'learning_rate': 0.05398174657392375, 'n_estimators': 245, 'subsample_for_bin': 33246, 'min_split_gain': 0.008247842164138808, 'min_child_weight': 0.00226070404942164, 'min_child_samples': 98, 'colsample_bytree': 0.6267341611507831, 'reg_alpha': 0.15822591691356322, 'reg_lambda': 0.19825096325778524, 'boosting_type': 'goss'}. Best is trial 1 with value: 0.030294308124055536.\n",
      "Trial 1 finished with value: 0.030294308124055536 and parameters: {'num_leaves': 484, 'learning_rate': 0.05398174657392375, 'n_estimators': 245, 'subsample_for_bin': 33246, 'min_split_gain': 0.008247842164138808, 'min_child_weight': 0.00226070404942164, 'min_child_samples': 98, 'colsample_bytree': 0.6267341611507831, 'reg_alpha': 0.15822591691356322, 'reg_lambda': 0.19825096325778524, 'boosting_type': 'goss'}. Best is trial 1 with value: 0.030294308124055536.\n",
      "Trial 1 finished with value: 0.030294308124055536 and parameters: {'num_leaves': 484, 'learning_rate': 0.05398174657392375, 'n_estimators': 245, 'subsample_for_bin': 33246, 'min_split_gain': 0.008247842164138808, 'min_child_weight': 0.00226070404942164, 'min_child_samples': 98, 'colsample_bytree': 0.6267341611507831, 'reg_alpha': 0.15822591691356322, 'reg_lambda': 0.19825096325778524, 'boosting_type': 'goss'}. Best is trial 1 with value: 0.030294308124055536.\n",
      "Trial 1 finished with value: 0.030294308124055536 and parameters: {'num_leaves': 484, 'learning_rate': 0.05398174657392375, 'n_estimators': 245, 'subsample_for_bin': 33246, 'min_split_gain': 0.008247842164138808, 'min_child_weight': 0.00226070404942164, 'min_child_samples': 98, 'colsample_bytree': 0.6267341611507831, 'reg_alpha': 0.15822591691356322, 'reg_lambda': 0.19825096325778524, 'boosting_type': 'goss'}. Best is trial 1 with value: 0.030294308124055536.\n",
      "Trial 1 finished with value: 0.030294308124055536 and parameters: {'num_leaves': 484, 'learning_rate': 0.05398174657392375, 'n_estimators': 245, 'subsample_for_bin': 33246, 'min_split_gain': 0.008247842164138808, 'min_child_weight': 0.00226070404942164, 'min_child_samples': 98, 'colsample_bytree': 0.6267341611507831, 'reg_alpha': 0.15822591691356322, 'reg_lambda': 0.19825096325778524, 'boosting_type': 'goss'}. Best is trial 1 with value: 0.030294308124055536.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-29 09:20:06,528] Trial 2 finished with value: 0.47200834712527884 and parameters: {'num_leaves': 364, 'learning_rate': 0.0015364739270441077, 'n_estimators': 125, 'subsample_for_bin': 14656, 'min_split_gain': 0.08210890778892284, 'min_child_weight': 0.001012426666302464, 'min_child_samples': 42, 'colsample_bytree': 0.27785193060276014, 'reg_alpha': 0.02120669071442607, 'reg_lambda': 0.2572639529013939, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.030294308124055536.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 finished with value: 0.47200834712527884 and parameters: {'num_leaves': 364, 'learning_rate': 0.0015364739270441077, 'n_estimators': 125, 'subsample_for_bin': 14656, 'min_split_gain': 0.08210890778892284, 'min_child_weight': 0.001012426666302464, 'min_child_samples': 42, 'colsample_bytree': 0.27785193060276014, 'reg_alpha': 0.02120669071442607, 'reg_lambda': 0.2572639529013939, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.030294308124055536.\n",
      "Trial 2 finished with value: 0.47200834712527884 and parameters: {'num_leaves': 364, 'learning_rate': 0.0015364739270441077, 'n_estimators': 125, 'subsample_for_bin': 14656, 'min_split_gain': 0.08210890778892284, 'min_child_weight': 0.001012426666302464, 'min_child_samples': 42, 'colsample_bytree': 0.27785193060276014, 'reg_alpha': 0.02120669071442607, 'reg_lambda': 0.2572639529013939, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.030294308124055536.\n",
      "Trial 2 finished with value: 0.47200834712527884 and parameters: {'num_leaves': 364, 'learning_rate': 0.0015364739270441077, 'n_estimators': 125, 'subsample_for_bin': 14656, 'min_split_gain': 0.08210890778892284, 'min_child_weight': 0.001012426666302464, 'min_child_samples': 42, 'colsample_bytree': 0.27785193060276014, 'reg_alpha': 0.02120669071442607, 'reg_lambda': 0.2572639529013939, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.030294308124055536.\n",
      "Trial 2 finished with value: 0.47200834712527884 and parameters: {'num_leaves': 364, 'learning_rate': 0.0015364739270441077, 'n_estimators': 125, 'subsample_for_bin': 14656, 'min_split_gain': 0.08210890778892284, 'min_child_weight': 0.001012426666302464, 'min_child_samples': 42, 'colsample_bytree': 0.27785193060276014, 'reg_alpha': 0.02120669071442607, 'reg_lambda': 0.2572639529013939, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.030294308124055536.\n",
      "Trial 2 finished with value: 0.47200834712527884 and parameters: {'num_leaves': 364, 'learning_rate': 0.0015364739270441077, 'n_estimators': 125, 'subsample_for_bin': 14656, 'min_split_gain': 0.08210890778892284, 'min_child_weight': 0.001012426666302464, 'min_child_samples': 42, 'colsample_bytree': 0.27785193060276014, 'reg_alpha': 0.02120669071442607, 'reg_lambda': 0.2572639529013939, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.030294308124055536.\n",
      "Trial 2 finished with value: 0.47200834712527884 and parameters: {'num_leaves': 364, 'learning_rate': 0.0015364739270441077, 'n_estimators': 125, 'subsample_for_bin': 14656, 'min_split_gain': 0.08210890778892284, 'min_child_weight': 0.001012426666302464, 'min_child_samples': 42, 'colsample_bytree': 0.27785193060276014, 'reg_alpha': 0.02120669071442607, 'reg_lambda': 0.2572639529013939, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.030294308124055536.\n",
      "Trial 2 finished with value: 0.47200834712527884 and parameters: {'num_leaves': 364, 'learning_rate': 0.0015364739270441077, 'n_estimators': 125, 'subsample_for_bin': 14656, 'min_split_gain': 0.08210890778892284, 'min_child_weight': 0.001012426666302464, 'min_child_samples': 42, 'colsample_bytree': 0.27785193060276014, 'reg_alpha': 0.02120669071442607, 'reg_lambda': 0.2572639529013939, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.030294308124055536.\n",
      "Trial 2 finished with value: 0.47200834712527884 and parameters: {'num_leaves': 364, 'learning_rate': 0.0015364739270441077, 'n_estimators': 125, 'subsample_for_bin': 14656, 'min_split_gain': 0.08210890778892284, 'min_child_weight': 0.001012426666302464, 'min_child_samples': 42, 'colsample_bytree': 0.27785193060276014, 'reg_alpha': 0.02120669071442607, 'reg_lambda': 0.2572639529013939, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.030294308124055536.\n",
      "Trial 2 finished with value: 0.47200834712527884 and parameters: {'num_leaves': 364, 'learning_rate': 0.0015364739270441077, 'n_estimators': 125, 'subsample_for_bin': 14656, 'min_split_gain': 0.08210890778892284, 'min_child_weight': 0.001012426666302464, 'min_child_samples': 42, 'colsample_bytree': 0.27785193060276014, 'reg_alpha': 0.02120669071442607, 'reg_lambda': 0.2572639529013939, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.030294308124055536.\n",
      "Trial 2 finished with value: 0.47200834712527884 and parameters: {'num_leaves': 364, 'learning_rate': 0.0015364739270441077, 'n_estimators': 125, 'subsample_for_bin': 14656, 'min_split_gain': 0.08210890778892284, 'min_child_weight': 0.001012426666302464, 'min_child_samples': 42, 'colsample_bytree': 0.27785193060276014, 'reg_alpha': 0.02120669071442607, 'reg_lambda': 0.2572639529013939, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.030294308124055536.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-29 09:21:32,228] Trial 3 finished with value: 0.030258329135784745 and parameters: {'num_leaves': 531, 'learning_rate': 0.08461439300203369, 'n_estimators': 488, 'subsample_for_bin': 24375, 'min_split_gain': 0.00793471150457945, 'min_child_weight': 0.000972639359594038, 'min_child_samples': 23, 'colsample_bytree': 0.41843497258937146, 'reg_alpha': 0.8173042783935529, 'reg_lambda': 0.352022477842215, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 finished with value: 0.030258329135784745 and parameters: {'num_leaves': 531, 'learning_rate': 0.08461439300203369, 'n_estimators': 488, 'subsample_for_bin': 24375, 'min_split_gain': 0.00793471150457945, 'min_child_weight': 0.000972639359594038, 'min_child_samples': 23, 'colsample_bytree': 0.41843497258937146, 'reg_alpha': 0.8173042783935529, 'reg_lambda': 0.352022477842215, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n",
      "Trial 3 finished with value: 0.030258329135784745 and parameters: {'num_leaves': 531, 'learning_rate': 0.08461439300203369, 'n_estimators': 488, 'subsample_for_bin': 24375, 'min_split_gain': 0.00793471150457945, 'min_child_weight': 0.000972639359594038, 'min_child_samples': 23, 'colsample_bytree': 0.41843497258937146, 'reg_alpha': 0.8173042783935529, 'reg_lambda': 0.352022477842215, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n",
      "Trial 3 finished with value: 0.030258329135784745 and parameters: {'num_leaves': 531, 'learning_rate': 0.08461439300203369, 'n_estimators': 488, 'subsample_for_bin': 24375, 'min_split_gain': 0.00793471150457945, 'min_child_weight': 0.000972639359594038, 'min_child_samples': 23, 'colsample_bytree': 0.41843497258937146, 'reg_alpha': 0.8173042783935529, 'reg_lambda': 0.352022477842215, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n",
      "Trial 3 finished with value: 0.030258329135784745 and parameters: {'num_leaves': 531, 'learning_rate': 0.08461439300203369, 'n_estimators': 488, 'subsample_for_bin': 24375, 'min_split_gain': 0.00793471150457945, 'min_child_weight': 0.000972639359594038, 'min_child_samples': 23, 'colsample_bytree': 0.41843497258937146, 'reg_alpha': 0.8173042783935529, 'reg_lambda': 0.352022477842215, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n",
      "Trial 3 finished with value: 0.030258329135784745 and parameters: {'num_leaves': 531, 'learning_rate': 0.08461439300203369, 'n_estimators': 488, 'subsample_for_bin': 24375, 'min_split_gain': 0.00793471150457945, 'min_child_weight': 0.000972639359594038, 'min_child_samples': 23, 'colsample_bytree': 0.41843497258937146, 'reg_alpha': 0.8173042783935529, 'reg_lambda': 0.352022477842215, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n",
      "Trial 3 finished with value: 0.030258329135784745 and parameters: {'num_leaves': 531, 'learning_rate': 0.08461439300203369, 'n_estimators': 488, 'subsample_for_bin': 24375, 'min_split_gain': 0.00793471150457945, 'min_child_weight': 0.000972639359594038, 'min_child_samples': 23, 'colsample_bytree': 0.41843497258937146, 'reg_alpha': 0.8173042783935529, 'reg_lambda': 0.352022477842215, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n",
      "Trial 3 finished with value: 0.030258329135784745 and parameters: {'num_leaves': 531, 'learning_rate': 0.08461439300203369, 'n_estimators': 488, 'subsample_for_bin': 24375, 'min_split_gain': 0.00793471150457945, 'min_child_weight': 0.000972639359594038, 'min_child_samples': 23, 'colsample_bytree': 0.41843497258937146, 'reg_alpha': 0.8173042783935529, 'reg_lambda': 0.352022477842215, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n",
      "Trial 3 finished with value: 0.030258329135784745 and parameters: {'num_leaves': 531, 'learning_rate': 0.08461439300203369, 'n_estimators': 488, 'subsample_for_bin': 24375, 'min_split_gain': 0.00793471150457945, 'min_child_weight': 0.000972639359594038, 'min_child_samples': 23, 'colsample_bytree': 0.41843497258937146, 'reg_alpha': 0.8173042783935529, 'reg_lambda': 0.352022477842215, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n",
      "Trial 3 finished with value: 0.030258329135784745 and parameters: {'num_leaves': 531, 'learning_rate': 0.08461439300203369, 'n_estimators': 488, 'subsample_for_bin': 24375, 'min_split_gain': 0.00793471150457945, 'min_child_weight': 0.000972639359594038, 'min_child_samples': 23, 'colsample_bytree': 0.41843497258937146, 'reg_alpha': 0.8173042783935529, 'reg_lambda': 0.352022477842215, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n",
      "Trial 3 finished with value: 0.030258329135784745 and parameters: {'num_leaves': 531, 'learning_rate': 0.08461439300203369, 'n_estimators': 488, 'subsample_for_bin': 24375, 'min_split_gain': 0.00793471150457945, 'min_child_weight': 0.000972639359594038, 'min_child_samples': 23, 'colsample_bytree': 0.41843497258937146, 'reg_alpha': 0.8173042783935529, 'reg_lambda': 0.352022477842215, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-29 09:24:26,943] Trial 4 finished with value: 0.054094408865222765 and parameters: {'num_leaves': 673, 'learning_rate': 0.0021334151746530508, 'n_estimators': 726, 'subsample_for_bin': 20099, 'min_split_gain': 0.025434599459215758, 'min_child_weight': 0.00017017889295294726, 'min_child_samples': 53, 'colsample_bytree': 0.8383564042010848, 'reg_alpha': 0.9033506632164597, 'reg_lambda': 0.7456918975242166, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 finished with value: 0.054094408865222765 and parameters: {'num_leaves': 673, 'learning_rate': 0.0021334151746530508, 'n_estimators': 726, 'subsample_for_bin': 20099, 'min_split_gain': 0.025434599459215758, 'min_child_weight': 0.00017017889295294726, 'min_child_samples': 53, 'colsample_bytree': 0.8383564042010848, 'reg_alpha': 0.9033506632164597, 'reg_lambda': 0.7456918975242166, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n",
      "Trial 4 finished with value: 0.054094408865222765 and parameters: {'num_leaves': 673, 'learning_rate': 0.0021334151746530508, 'n_estimators': 726, 'subsample_for_bin': 20099, 'min_split_gain': 0.025434599459215758, 'min_child_weight': 0.00017017889295294726, 'min_child_samples': 53, 'colsample_bytree': 0.8383564042010848, 'reg_alpha': 0.9033506632164597, 'reg_lambda': 0.7456918975242166, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n",
      "Trial 4 finished with value: 0.054094408865222765 and parameters: {'num_leaves': 673, 'learning_rate': 0.0021334151746530508, 'n_estimators': 726, 'subsample_for_bin': 20099, 'min_split_gain': 0.025434599459215758, 'min_child_weight': 0.00017017889295294726, 'min_child_samples': 53, 'colsample_bytree': 0.8383564042010848, 'reg_alpha': 0.9033506632164597, 'reg_lambda': 0.7456918975242166, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n",
      "Trial 4 finished with value: 0.054094408865222765 and parameters: {'num_leaves': 673, 'learning_rate': 0.0021334151746530508, 'n_estimators': 726, 'subsample_for_bin': 20099, 'min_split_gain': 0.025434599459215758, 'min_child_weight': 0.00017017889295294726, 'min_child_samples': 53, 'colsample_bytree': 0.8383564042010848, 'reg_alpha': 0.9033506632164597, 'reg_lambda': 0.7456918975242166, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n",
      "Trial 4 finished with value: 0.054094408865222765 and parameters: {'num_leaves': 673, 'learning_rate': 0.0021334151746530508, 'n_estimators': 726, 'subsample_for_bin': 20099, 'min_split_gain': 0.025434599459215758, 'min_child_weight': 0.00017017889295294726, 'min_child_samples': 53, 'colsample_bytree': 0.8383564042010848, 'reg_alpha': 0.9033506632164597, 'reg_lambda': 0.7456918975242166, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n",
      "Trial 4 finished with value: 0.054094408865222765 and parameters: {'num_leaves': 673, 'learning_rate': 0.0021334151746530508, 'n_estimators': 726, 'subsample_for_bin': 20099, 'min_split_gain': 0.025434599459215758, 'min_child_weight': 0.00017017889295294726, 'min_child_samples': 53, 'colsample_bytree': 0.8383564042010848, 'reg_alpha': 0.9033506632164597, 'reg_lambda': 0.7456918975242166, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n",
      "Trial 4 finished with value: 0.054094408865222765 and parameters: {'num_leaves': 673, 'learning_rate': 0.0021334151746530508, 'n_estimators': 726, 'subsample_for_bin': 20099, 'min_split_gain': 0.025434599459215758, 'min_child_weight': 0.00017017889295294726, 'min_child_samples': 53, 'colsample_bytree': 0.8383564042010848, 'reg_alpha': 0.9033506632164597, 'reg_lambda': 0.7456918975242166, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n",
      "Trial 4 finished with value: 0.054094408865222765 and parameters: {'num_leaves': 673, 'learning_rate': 0.0021334151746530508, 'n_estimators': 726, 'subsample_for_bin': 20099, 'min_split_gain': 0.025434599459215758, 'min_child_weight': 0.00017017889295294726, 'min_child_samples': 53, 'colsample_bytree': 0.8383564042010848, 'reg_alpha': 0.9033506632164597, 'reg_lambda': 0.7456918975242166, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n",
      "Trial 4 finished with value: 0.054094408865222765 and parameters: {'num_leaves': 673, 'learning_rate': 0.0021334151746530508, 'n_estimators': 726, 'subsample_for_bin': 20099, 'min_split_gain': 0.025434599459215758, 'min_child_weight': 0.00017017889295294726, 'min_child_samples': 53, 'colsample_bytree': 0.8383564042010848, 'reg_alpha': 0.9033506632164597, 'reg_lambda': 0.7456918975242166, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n",
      "Trial 4 finished with value: 0.054094408865222765 and parameters: {'num_leaves': 673, 'learning_rate': 0.0021334151746530508, 'n_estimators': 726, 'subsample_for_bin': 20099, 'min_split_gain': 0.025434599459215758, 'min_child_weight': 0.00017017889295294726, 'min_child_samples': 53, 'colsample_bytree': 0.8383564042010848, 'reg_alpha': 0.9033506632164597, 'reg_lambda': 0.7456918975242166, 'boosting_type': 'gbdt'}. Best is trial 3 with value: 0.030258329135784745.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys \n",
    "\n",
    "def get_suggestion(func, param, kwargs):\n",
    "    def _sugg(trial):\n",
    "        return getattr(trial, func)(param, **kwargs)\n",
    "    return _sugg\n",
    "\n",
    "lgmb_param_set = {\n",
    "    \"num_leaves\": get_suggestion(\"suggest_int\", \"num_leaves\", {\"low\": 63, \"high\":1023}), \n",
    "    \"learning_rate\": get_suggestion(\"suggest_float\", \"learning_rate\", {\"low\": 0.001, \"high\": 1, \"log\": True}), \n",
    "    \"n_estimators\": get_suggestion(\"suggest_int\", \"n_estimators\", {\"low\": 10,  \"high\": 1000, \"log\": True}), \n",
    "    \"subsample_for_bin\": get_suggestion(\"suggest_int\", \"subsample_for_bin\", {\"low\": 10000, \"high\": 50000, \"log\": True}), \n",
    "    \"min_split_gain\": get_suggestion(\"suggest_float\", \"min_split_gain\", {\"low\": 0.0, \"high\": 0.1}), \n",
    "    \"min_child_weight\": get_suggestion(\"suggest_float\",\"min_child_weight\", {\"low\": .0001, \"high\": 0.01, \"log\": True}),\n",
    "    \"min_child_samples\": get_suggestion(\"suggest_int\", \"min_child_samples\", {\"low\": 10, \"high\": 100}),\n",
    "    \"colsample_bytree\": get_suggestion(\"suggest_float\", \"colsample_bytree\", {\"low\": 0.1, \"high\": 1.0}), \n",
    "    \"reg_alpha\": get_suggesion(\"suggest_float\", \"reg_alpha\", {\"low\": 0.0, \"high\": 0.99}), \n",
    "    \"reg_lambda\": get_suggestion(\"suggest_float\", \"reg_lambda\", {\"low\": 0.0, \"high\": 0.99}),\n",
    "    \"boosting_type\": get_suggestion(\"suggest_categorical\", \"boosting_type\", {\"choices\": ['gbdt', 'dart', 'goss']})\n",
    "}\n",
    "\n",
    "def load_dataset(csv_path, split_ratio=0.8):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.loc[:, ~df.columns.str.contains(\"unnamed\", case=False)]\n",
    "    sample_cnt = df.shape[0]\n",
    "    train_eval_split = int(split_ratio * sample_cnt)\n",
    "\n",
    "    train_sample = df.iloc[:train_eval_split, :]\n",
    "    test_sample = df.iloc[train_eval_split:, :]\n",
    "    def split_xy(df:pd.DataFrame):\n",
    "        label_col_flags = df.columns.str.find(\"labels\") == 0\n",
    "        assert sum(label_col_flags) == 1\n",
    "        x = df.loc[:, ~label_col_flags]\n",
    "        y = df.loc[:, label_col_flags]\n",
    "        return x, y.labels\n",
    "    train_x, train_y = split_xy(train_sample)\n",
    "    eval_x, eval_y = split_xy(test_sample)\n",
    "    return train_x, train_y, eval_x, eval_y\n",
    "\n",
    "def objective(trial):\n",
    "    train_x, train_y, eval_x, eval_y = load_dataset(\"278k_song_labelled.csv\")\n",
    "    param_set = {}\n",
    "    for p, s in lgmb_param_set.items():\n",
    "        param_set[p] = s(trial)\n",
    "\n",
    "    lgbm_classifier = lgbm.LGBMClassifier(objective='multiclass', **param_set)\n",
    "    lgbm_classifier.fit(train_x, train_y)\n",
    "    pred_y = lgbm_classifier.predict(eval_x)\n",
    "    loss = sklearn_metric_loss_score(\"accuracy\", pred_y, eval_y)\n",
    "    trial.report(loss, param_set['n_estimators'])\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study = optuna.create_study(pruner=optuna.pruners.HyperbandPruner())\n",
    "study.optimize(objective, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'num_leaves': 531,\n",
       "  'learning_rate': 0.08461439300203369,\n",
       "  'n_estimators': 488,\n",
       "  'subsample_for_bin': 24375,\n",
       "  'min_split_gain': 0.00793471150457945,\n",
       "  'min_child_weight': 0.000972639359594038,\n",
       "  'min_child_samples': 23,\n",
       "  'colsample_bytree': 0.41843497258937146,\n",
       "  'reg_alpha': 0.8173042783935529,\n",
       "  'reg_lambda': 0.352022477842215,\n",
       "  'boosting_type': 'gbdt'},\n",
       " 0.9697416708642153)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params, 1-study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_estimators': 136,\n",
       "  'num_leaves': 690,\n",
       "  'min_child_samples': 31,\n",
       "  'learning_rate': 0.7557849801382729,\n",
       "  'log_max_bin': 10,\n",
       "  'colsample_bytree': 1.0,\n",
       "  'reg_alpha': 0.0009765625,\n",
       "  'reg_lambda': 0.007756568049822362},\n",
       " 0.957113045981147)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_flaml2.best_config, 1-lgbm_flaml2.best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lgbm'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
